{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f1649e-15a9-492f-947c-e72fe9bb2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oliven/miniconda3/envs/geneformer2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import anndata as an\n",
    "import scanpy as sc\n",
    "import os\n",
    "import gc\n",
    "from importlib import reload\n",
    "\n",
    "from datasets import Dataset, load_from_disk\n",
    "from datasets import load_dataset\n",
    "from geneformer import EmbExtractor\n",
    "\n",
    "# classifer tools\n",
    "import xgboost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import anndata as an\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "\n",
    "from datasets import Dataset, load_from_disk, load_dataset\n",
    "import geneformer\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "DEFAULT_NAME_PATH = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer/gene_name_id_dict.pkl\"\n",
    "DEFAULT_TOKEN_PATH = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/token_dictionary.pkl\"\n",
    "DEFAULT_MEDIAN_PATH = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer/gene_median_dictionary.pkl\"\n",
    "\n",
    "sns.set_style('white')\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e8f9f1c-6642-445d-b841-3a812f2540ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at /scratch/indikar_root/indikar1/shared_data/geneformer/fine_tune/240715_geneformer_cellClassifier_no_induced/ksplit1/ and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded!\n"
     ]
    }
   ],
   "source": [
    "### Parameters v\n",
    "###\n",
    "\n",
    "# cells to filter on\n",
    "initial_cell_type = 'Fibroblast'\n",
    "\n",
    "# list of genes to perturb with at the front of the list\n",
    "gene_list = [\n",
    "    'GATA2', \n",
    "    'GFI1B', \n",
    "    'FOS', \n",
    "    'STAT5A',\n",
    "    'REL',\n",
    "    'FOSB',\n",
    "    'IKZF1',\n",
    "    'RUNX3',\n",
    "    'MEF2C',\n",
    "    'ETV6',\n",
    "]\n",
    "\n",
    "############### important! ###############\n",
    "# added back sampling, now all cells\n",
    "num_initial_cells = 10\n",
    "################  (1/2)  #################\n",
    "MODEL_PATH = \"/scratch/indikar_root/indikar1/shared_data/geneformer/fine_tune/240715_geneformer_cellClassifier_no_induced/ksplit1/\"\n",
    "#     Args:\n",
    "#         MODEL_PATH (str): Path to the model file.\n",
    "#         model_type (str, optional): Type of model ('Pretrained' or custom). Default: 'Pretrained'.\n",
    "#         n_classes (int, optional): Number of output classes for custom models. Default: 0.\n",
    "#         mode (str, optional): Mode to load the model in ('eval' or 'train'). Default: 'eval'.\n",
    "model = geneformer.perturber_utils.load_model('Pretrained', 0 , MODEL_PATH, 'eval')\n",
    "\n",
    "\n",
    "print('model loaded!')\n",
    "\n",
    "\n",
    "TOKEN_DATA_PATH = \"/scratch/indikar_root/indikar1/shared_data/geneformer/resources/token_mapping.csv\"\n",
    "token_df = pd.read_csv(TOKEN_DATA_PATH)\n",
    "\n",
    "\n",
    "DATA_PATH = \"/scratch/indikar_root/indikar1/shared_data/geneformer/fine_tune/hsc.dataset\"\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "### Parameters ^"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4be7d-190b-4234-a830-d0e26e5caba9",
   "metadata": {},
   "source": [
    "# Load In Things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fadf0696-57d7-4527-a06f-49f4f47fcb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses token_df to translate from gene_list to tokens_list v\n",
    "def get_tokens_list(gene_list):\n",
    "    # Get a df of the genes we are perturbing with\n",
    "    genes = token_df[token_df['gene_name'].isin(gene_list)]\n",
    "    \n",
    "    tf_map = dict(zip(genes['gene_name'].values, genes['token_id'].values))\n",
    "    \n",
    "    # Create tokens_list by looking up each gene_name in the tf_map\n",
    "    tokens_list = [tf_map.get(gene_name, gene_name) for gene_name in gene_list]\n",
    "\n",
    "    return tokens_list\n",
    "\n",
    "\n",
    "# def load_model(MODEL_PATH, model_type='Pretrained', n_classes=0, mode='eval'):\n",
    "#     \"\"\"\n",
    "#     Loads a pre-trained or custom model for geneformer perturbations.\n",
    "\n",
    "#     Args:\n",
    "#         MODEL_PATH (str): Path to the model file.\n",
    "#         model_type (str, optional): Type of model ('Pretrained' or custom). Default: 'Pretrained'.\n",
    "#         n_classes (int, optional): Number of output classes for custom models. Default: 0.\n",
    "#         mode (str, optional): Mode to load the model in ('eval' or 'train'). Default: 'eval'.\n",
    "\n",
    "#     Returns:\n",
    "#         The loaded model object.\n",
    "#     \"\"\"\n",
    "\n",
    "#     model = geneformer.perturber_utils.load_model(\n",
    "#         model_type,\n",
    "#         n_classes,\n",
    "#         MODEL_PATH,\n",
    "#         mode\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "def add_perturbations_to_cell(cell_tokens, perturbation_tokens):\n",
    "    \"\"\"\n",
    "    Requires: cell_tokens is a list of (in our use, 2048) integer tokens, each token representing a gene,\n",
    "        perturbation_tokens is a list of integer tokens\n",
    "    Effects: returns final_tokens, a list of tokens with perturbation_tokens at the front\n",
    "    \"\"\"\n",
    "    original_length = len(cell_tokens)\n",
    "\n",
    "    # Remove existing perturbation tokens from the cell\n",
    "    cell_tokens = [token for token in cell_tokens if token not in perturbation_tokens]\n",
    "\n",
    "    # Add perturbations, then slice or pad to match original length\n",
    "    final_tokens = (perturbation_tokens + cell_tokens)[:original_length]  # Slice if too long\n",
    "    final_tokens += [0] * (original_length - len(final_tokens))            # Pad if too short\n",
    "\n",
    "    return final_tokens\n",
    "\n",
    "\n",
    "def extract_embedding_in_mem(model, data, emb_mode='cell', layer_to_quant=-1, forward_batch_size=10):\n",
    "    \"\"\"Extracts embeddings from a model and returns them as a DataFrame.\n",
    "\n",
    "    This function provides an in-memory extraction of embeddings, allowing for convenient\n",
    "    manipulation and analysis directly within your Python environment.\n",
    "\n",
    "    Args:\n",
    "        model: The model to use for embedding extraction.\n",
    "        data: The input data for which embeddings need to be extracted.\n",
    "        emb_mode (str, optional): The embedding mode. Defaults to 'cell'.\n",
    "        layer_to_quant (int, optional): The layer to quantize. Defaults to -1 (last layer).\n",
    "        forward_batch_size (int, optional): The batch size for forward passes. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the extracted embeddings.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If `model` is not a supported model type.\n",
    "        ValueError: If `data` is not in the correct format.\n",
    "    \"\"\"\n",
    "\n",
    "    embs = geneformer.emb_extractor.get_embs(\n",
    "        model,\n",
    "        data,\n",
    "        emb_mode,\n",
    "        layer_to_quant,\n",
    "        0,  # Assuming this is a constant parameter for the function\n",
    "        forward_batch_size,\n",
    "        summary_stat=None,  \n",
    "        silent=False, \n",
    "    )\n",
    "    data = embs.cpu().numpy()\n",
    "    if emb_mode=='cell':\n",
    "        return pd.DataFrame(data)\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "def embedding_to_adata(df: pd.DataFrame, n_dim: int = None) -> an.AnnData:\n",
    "    \"\"\"Converts a Pandas DataFrame with an embedding to an AnnData object.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame with numerical embedding columns and optional metadata columns.\n",
    "        n_dim: The number of dimensions to keep in the embedding. If None, all dimensions are kept.\n",
    "\n",
    "    Returns:\n",
    "        The converted AnnData object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `n_dim` exceeds the available dimensions in the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    if n_dim is not None and n_dim > df.shape[1]:\n",
    "        raise ValueError(f\"n_dim ({n_dim}) exceeds available dimensions ({df.shape[1]})\")\n",
    "\n",
    "    # Assuming embedding columns are those that are not integers\n",
    "    is_metadata = df.columns.astype(str).str.isdigit()\n",
    "    metadata_df = df.loc[:, ~is_metadata]\n",
    "    embedding_df = df.loc[:, is_metadata]\n",
    "\n",
    "    cell_index = pd.Index([f\"C{x}\" for x in range(df.shape[0])], name='obs_names')\n",
    "\n",
    "    if n_dim is not None:\n",
    "        embedding_df = embedding_df.iloc[:, :n_dim]\n",
    "\n",
    "    var_index = pd.Index([f\"D{x}\" for x in range(embedding_df.shape[1])], name='var_names')\n",
    "\n",
    "    adata = an.AnnData(embedding_df.to_numpy())\n",
    "    adata.obs_names = cell_index\n",
    "    adata.var_names = var_index\n",
    "    adata.obs = metadata_df\n",
    "    return adata\n",
    "\n",
    "\"\"\"\n",
    "For us only!!!! vvv\n",
    "\"\"\"\n",
    "def ten_choose_five():\n",
    "    gene_list = [\n",
    "    'GATA2', \n",
    "    'GFI1B', \n",
    "    'FOS', \n",
    "    'STAT5A',\n",
    "    'REL',\n",
    "    'FOSB',\n",
    "    'IKZF1',\n",
    "    'RUNX3',\n",
    "    'MEF2C',\n",
    "    'ETV6',\n",
    "]\n",
    "# Define the length of sublists\n",
    "    len_sublist = 5\n",
    "    \n",
    "    # Generate all combinations of the specified length\n",
    "    sublists = list(combinations(gene_list, len_sublist))\n",
    "    \n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'recipe_iteration': range(1, len(sublists) + 1),\n",
    "        'recipe_list': [list(sublist) for sublist in sublists]\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Check and ensure all data is of correct type\n",
    "def check_and_convert(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for col in data.columns:\n",
    "            if not pd.api.types.is_string_dtype(data[col]):\n",
    "                data[col] = data[col].astype(str)\n",
    "    return data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b9eb9f-6aee-4ba2-a09d-aabf0c5d676a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available! Devices:  1\n",
      "Current CUDA device:  0\n",
      "Device name:  NVIDIA A100 80GB PCIe MIG 3g.40gb\n"
     ]
    }
   ],
   "source": [
    "### Preliminaries v\n",
    "###\n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"CUDA is available! Devices: \", torch.cuda.device_count()) \n",
    "    print(\"Current CUDA device: \", torch.cuda.current_device()) \n",
    "    print(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device())) \n",
    "else: print(\"CUDA is not available\")\n",
    "\n",
    "\n",
    "\n",
    "###\n",
    "### Preliminaries ^\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # jobNumber = int(sys.argv[1])\n",
    "    # print(jobNumber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b23854fd-3b9b-49e8-a81a-57f33e6f07c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Format raw data, print messages to check v\n",
    "# ###\n",
    "\n",
    "# # Load from pre-trained data\n",
    "# raw_data = load_from_disk(DATA_PATH)\n",
    "\n",
    "# # Convert to DataFrame for filtering\n",
    "# df = raw_data.to_pandas()\n",
    "# print(\"\\nOriginal Dataset:\")\n",
    "# print(f\"  - Number of samples: {df.shape[0]:,}\")\n",
    "# print(f\"  - Number of columns: {df.shape[1]:,}\")\n",
    "\n",
    "# # Filtering\n",
    "# fb_df = df[df['standardized_cell_type'] == initial_cell_type]\n",
    "\n",
    "# ############### important! ###############\n",
    "# # sampling (ADDED BACK!)\n",
    "# fb_df = fb_df.sample(num_initial_cells)\n",
    "# ################  (2/2)  #################\n",
    "\n",
    "# fb_df = fb_df.reset_index(drop=True)\n",
    "\n",
    "# # add a cell id\n",
    "# fb_df['cell_id'] = [f\"cell_{i+1}\" for i in range(len(fb_df))]\n",
    "# fb_df['recipe'] = 'raw'  # as opposed to having a speciofic ;-separated recipe list. other entries will have this.\n",
    "# fb_df['type'] = 'initial' # this dataframe no longer includes 'target'\n",
    "\n",
    "# print(\"\\nFiltered Dataset:\")\n",
    "# print(f\"  - Number of samples: {fb_df.shape[0]:,}\")   # Nicer formatting with commas\n",
    "# print(f\"  - Number of columns: {fb_df.shape[1]:,}\")\n",
    "\n",
    "# # Value counts with sorting\n",
    "# print(\"\\nCell Type Distribution (Filtered):\")\n",
    "# print(fb_df['standardized_cell_type'].value_counts().sort_index())  # Sort for readability\n",
    "\n",
    "# # Convert back to Dataset\n",
    "# fb_data = Dataset.from_pandas(fb_df)\n",
    "# print(f\"\\nDataset converted back: {fb_data}\")\n",
    "\n",
    "# ###\n",
    "# ### Format raw data, print messages to check ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c80bbf-a91a-4e00-84b5-21ea28515d7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# ### Perform the perturbation v\n",
    "# ###\n",
    "\n",
    "# reprogramming_df = [\n",
    "#     fb_df\n",
    "# ]\n",
    "\n",
    "# perturb = fb_df.copy()\n",
    "# recipe = \";\".join(gene_list)\n",
    "# perturb['recipe'] = recipe\n",
    "# perturb['type'] = 'reprogrammed'\n",
    "# perturb['input_ids'] = perturb['input_ids'].apply(lambda x: add_perturbations_to_cell(x, get_tokens_list(gene_list)))\n",
    "\n",
    "# reprogramming_df.append(perturb)\n",
    "\n",
    "# reprogramming_df = pd.concat(reprogramming_df, ignore_index=True)\n",
    "\n",
    "# print(f\"{reprogramming_df.shape=}\")\n",
    "# reprogramming_df.sample(10)\n",
    "\n",
    "# ###\n",
    "# ### Perform the perturbation ^\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88646512-aed1-477d-9bf7-3af8ec9efd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# reprogramming_data = Dataset.from_pandas(reprogramming_df)\n",
    "\n",
    "# reprogramming_embs = extract_embedding_in_mem(\n",
    "#     model, \n",
    "#     reprogramming_data, \n",
    "#     layer_to_quant=-1,\n",
    "#     forward_batch_size=100,\n",
    "# )\n",
    "# print(f\"{reprogramming_embs.shape=}\")\n",
    "\n",
    "# # translate into an anndata object and plot\n",
    "# reprogramming_adata = embedding_to_adata(reprogramming_embs)\n",
    "# reprogramming_adata.obs = reprogramming_df.copy()\n",
    "\n",
    "# # sc.tl.pca(reprogramming_adata, n_comps=25)\n",
    "# # sc.pp.neighbors(reprogramming_adata, n_neighbors=200)\n",
    "# # sc.tl.umap(reprogramming_adata, \n",
    "# #            min_dist=0.75,\n",
    "# #           )\n",
    "\n",
    "# reprogramming_adata.obs.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfaa622b-580e-4ddb-8a0a-43f69ba1c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reprogramming_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2708f6c-b4cd-406d-8f63-a75e0a10f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draft of single perturbation:\n",
    "def single_recipe(recipe_list):\n",
    "    ### Format raw data, print messages to check v\n",
    "    ###\n",
    "\n",
    "    # Load from pre-trained data\n",
    "    raw_data = load_from_disk(DATA_PATH)\n",
    "\n",
    "    # Convert to DataFrame for filtering\n",
    "    df = raw_data.to_pandas()\n",
    "    print(\"\\nOriginal Dataset:\")\n",
    "    print(f\"  - Number of samples: {df.shape[0]:,}\")\n",
    "    print(f\"  - Number of columns: {df.shape[1]:,}\")\n",
    "\n",
    "    # Filtering\n",
    "    fb_df = df[df['standardized_cell_type'] == initial_cell_type]\n",
    "\n",
    "    ############### important! ###############\n",
    "    # sampling (ADDED BACK!)\n",
    "    fb_df = fb_df.sample(num_initial_cells)\n",
    "    ################  (2/2)  #################\n",
    "\n",
    "    fb_df = fb_df.reset_index(drop=True)\n",
    "\n",
    "    # add a cell id\n",
    "    fb_df['cell_id'] = [f\"cell_{i+1}\" for i in range(len(fb_df))]\n",
    "    fb_df['recipe'] = 'raw'  # as opposed to having a speciofic ;-separated recipe list. other entries will have this.\n",
    "    fb_df['type'] = 'initial' # this dataframe no longer includes 'target'\n",
    "\n",
    "    print(\"\\nFiltered Dataset:\")\n",
    "    print(f\"  - Number of samples: {fb_df.shape[0]:,}\")   # Nicer formatting with commas\n",
    "    print(f\"  - Number of columns: {fb_df.shape[1]:,}\")\n",
    "\n",
    "    # Value counts with sorting\n",
    "    print(\"\\nCell Type Distribution (Filtered):\")\n",
    "    print(fb_df['standardized_cell_type'].value_counts().sort_index())  # Sort for readability\n",
    "\n",
    "    # Convert back to Dataset\n",
    "    fb_data = Dataset.from_pandas(fb_df)\n",
    "    print(f\"\\nDataset converted back: {fb_data}\")\n",
    "\n",
    "    ###\n",
    "    ### Format raw data, print messages to check ^\n",
    "\n",
    "    ############################################################################\n",
    "    \n",
    "    ### Perform the perturbation v\n",
    "    ###\n",
    "\n",
    "    reprogramming_df = [\n",
    "        fb_df\n",
    "    ]\n",
    "\n",
    "    perturb = fb_df.copy()\n",
    "    recipe = \";\".join(recipe_list)\n",
    "    perturb['recipe'] = recipe\n",
    "    perturb['type'] = 'reprogrammed'\n",
    "    perturb['input_ids'] = perturb['input_ids'].apply(lambda x: add_perturbations_to_cell(x, get_tokens_list(recipe_list)))\n",
    "\n",
    "    reprogramming_df.append(perturb)\n",
    "\n",
    "    reprogramming_df = pd.concat(reprogramming_df, ignore_index=True)\n",
    "\n",
    "    print(f\"{reprogramming_df.shape=}\")\n",
    "\n",
    "    ###\n",
    "    ### Perform the perturbation ^\n",
    "\n",
    "    ############################################################################\n",
    "    ### Get the embeddings into an Anndata object v\n",
    "    ###\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    reprogramming_data = Dataset.from_pandas(reprogramming_df)\n",
    "\n",
    "    reprogramming_embs = extract_embedding_in_mem(\n",
    "        model, \n",
    "        reprogramming_data, \n",
    "        layer_to_quant=-1,\n",
    "        forward_batch_size=100,\n",
    "    )\n",
    "    print(f\"{reprogramming_embs.shape=}\")\n",
    "\n",
    "    # translate into an anndata object and plot\n",
    "    reprogramming_adata = embedding_to_adata(reprogramming_embs)\n",
    "    reprogramming_adata.obs = reprogramming_df.copy()\n",
    "\n",
    "\n",
    "    reprogramming_adata.obs.head()\n",
    "\n",
    "    return reprogramming_adata\n",
    "    ###\n",
    "    ### Perform the perturbation ^\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbfb7f9-a78f-4a76-801c-81e9a0bf9915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Dataset:\n",
      "  - Number of samples: 214,715\n",
      "  - Number of columns: 8\n",
      "\n",
      "Filtered Dataset:\n",
      "  - Number of samples: 10\n",
      "  - Number of columns: 11\n",
      "\n",
      "Cell Type Distribution (Filtered):\n",
      "standardized_cell_type\n",
      "Fibroblast    10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset converted back: Dataset({\n",
      "    features: ['input_ids', 'cell_type', 'dataset', 'length', 'ignore', 'standardized_cell_type', 'broad_type', '__index_level_0__', 'cell_id', 'recipe', 'type'],\n",
      "    num_rows: 10\n",
      "})\n",
      "reprogramming_df.shape=(20, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reprogramming_embs.shape=(20, 512)\n",
      "File successfully written to /home/oliven/test_trash/jupyter_notebooks/2024-07-30_19-15-31_job_number_1.h5ad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# comment out parts as you add them to main.\n",
    "# don't add the globals or the prelims\n",
    "\n",
    "def main():\n",
    "    # load the model\n",
    "\n",
    "    jobNumber = 1\n",
    "    # For our iterations specifically...\n",
    "    all_input_recipes = ten_choose_five()\n",
    "    recipe_list = all_input_recipes.at[all_input_recipes[all_input_recipes['recipe_iteration'] == jobNumber].index[0], 'recipe_list']\n",
    "    \n",
    "\n",
    "    # run on a specific perturbation ( last thing not yet: change with ten_choose_five() )\n",
    "    reprogramming_adata = single_recipe(recipe_list)\n",
    "\n",
    "\n",
    "    reprogramming_adata.obs = check_and_convert(reprogramming_adata.obs)\n",
    "    reprogramming_adata.var = check_and_convert(reprogramming_adata.var)\n",
    "   \n",
    "        \n",
    "    filepath = f\"/home/oliven/test_trash/jupyter_notebooks/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_job_number_{jobNumber}.h5ad\"\n",
    "\n",
    "    try:\n",
    "        reprogramming_adata.write(filepath)\n",
    "        print(f\"File successfully written to {filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "    \n",
    "\n",
    "    # get and then return reprogramming_anndata\n",
    "\n",
    "    # save to a specific place ( user -> turbo. do both later)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c8914-6ee0-4cf7-b370-2b0c29ec8db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8be73f-bff8-4e14-bd6e-8288407045e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5035525-805a-4c89-9213-febd85facfe5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d333d13-68a3-405f-bdf7-5144c37fe274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0475c9-8050-4e97-802c-f0bd7955c032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69e1053-f006-4fa1-8175-d898c930c260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6602bc9-dff2-4a0e-a9a9-8199fa330264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19f9374-6191-496f-a3fd-0e692ca0b67d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd22604-c989-4ebd-8295-530145c82f5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47d1157-2627-4b3e-982c-16c6ec38048d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbef2e5-6197-4beb-b26f-d178a43bc889",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d2876-09ee-439c-a555-c7c359a6486f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dad88c-e50f-4785-a5e6-45533998342b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2bc0c-4a88-4a0d-b006-393aff41598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing string naming\n",
    "from datetime import datetime\n",
    "\n",
    "    # Get the current date and time\n",
    "now = datetime.now()\n",
    "    \n",
    "    # Format the date and time\n",
    "date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "\n",
    "jobNumber = 10\n",
    "filename = f\"{date_time_str}_job_number_{jobNumber}\"\n",
    "filepath = '/home/oliven/test_trash' + filename + '.h5ad'\n",
    "reprogramming_adata.write(filepath)\n",
    "\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cd84f5-6b0d-4263-8e9a-09fe848a90d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming reprogramming_adata is already defined and is a valid AnnData object\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "    \n",
    "# Format the date and time\n",
    "date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define jobNumber\n",
    "jobNumber = 10\n",
    "\n",
    "# Create the filename with \"_job_number_\" between date and jobNumber\n",
    "filename = f\"{date_time_str}_job_number_{jobNumber}.h5ad\"\n",
    "\n",
    "# Define the filepath\n",
    "filepath = f'/home/oliven/test_trash/{filename}'\n",
    "\n",
    "# Write the AnnData object to file\n",
    "reprogramming_adata.write(filepath)\n",
    "\n",
    "print(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2615ac17-3200-4128-974e-e3786b062b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Assuming reprogramming_adata is already defined and is a valid AnnData object\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "    \n",
    "# Format the date and time\n",
    "date_time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Define jobNumber\n",
    "jobNumber = 10\n",
    "\n",
    "# Create the filename with \"_job_number_\" between date and jobNumber\n",
    "filename = f\"{date_time_str}_job_number_{jobNumber}.h5ad\"\n",
    "\n",
    "# Define the filepath\n",
    "filepath = f'/home/oliven/test_trash/{filename}'\n",
    "\n",
    "# Check and ensure all data is of correct type\n",
    "def check_and_convert(data):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for col in data.columns:\n",
    "            if not pd.api.types.is_string_dtype(data[col]):\n",
    "                data[col] = data[col].astype(str)\n",
    "    return data\n",
    "\n",
    "reprogramming_adata.obs = check_and_convert(reprogramming_adata.obs)\n",
    "reprogramming_adata.var = check_and_convert(reprogramming_adata.var)\n",
    "\n",
    "# Write the AnnData object to file\n",
    "try:\n",
    "    reprogramming_adata.write(filepath)\n",
    "    print(f\"File successfully written to {filepath}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "\n",
    "print(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c70cbb-15cc-49b8-b34d-a1e8398c7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb72d036-bcfa-44b7-8eec-e1b2c3531bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = reprogramming_adata.obs[reprogramming_adata.obs['type'] == 'initial']\n",
    "target = reprogramming_adata.obs[reprogramming_adata.obs['type'] == 'target']\n",
    "repro = reprogramming_adata.obs[reprogramming_adata.obs['type'] == 'reprogrammed']\n",
    "\n",
    "# precompute all distances\n",
    "metric = 'cosine'\n",
    "D = squareform(pdist(reprogramming_adata.X, metric=metric))\n",
    "print(f\"{D.shape=}\")\n",
    "\n",
    "inital_to_target = D[initial.index, target.index].mean()\n",
    "print(f\"{inital_to_target=:.4f}\")\n",
    "\n",
    "result = []\n",
    "\n",
    "for i, (recipe, group) in enumerate(repro.groupby('recipe')):\n",
    "\n",
    "    # might want to \n",
    "    if i % 25 == 0:\n",
    "        print(f\"Recipe {i}/{len(inputs)}...\")\n",
    "    \n",
    "    # compute group to intial\n",
    "    recipe_to_initial = D[group.index, initial.index].mean() # average over all cells\n",
    "    \n",
    "    # compute group to target\n",
    "    recipe_to_target = D[group.index, target.index].mean() # average over all cells\n",
    "    \n",
    "    row = {\n",
    "        'recipe' : recipe,\n",
    "        'recipe_to_initial' : recipe_to_initial,\n",
    "        'recipe_to_target' : recipe_to_target,\n",
    "        'recipe_diff' : inital_to_target - recipe_to_target,\n",
    "    }\n",
    "    result.append(row)\n",
    "    \n",
    "result = pd.DataFrame(result)\n",
    "result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb39ff-4005-47d6-9675-61da2dfd165c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geneformer2",
   "language": "python",
   "name": "geneformer2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
