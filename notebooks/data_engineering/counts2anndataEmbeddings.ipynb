{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdc1918e-3a17-4302-a6bc-47cf9d377f88",
   "metadata": {},
   "source": [
    "# AnnData -> GeneFormerEmbeddings -> AnnData\n",
    "\n",
    "In this file Joshua changes a file made by cooper to standardize the time point and replicate naming from files made by cooper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1092857-4c6f-42f4-a640-840c0b0823d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5daa700d-ac65-46b2-b7ca-53aff724b553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import scipy.sparse as sp\n",
    "import scanpy as sc\n",
    "import anndata as an\n",
    "from datasets import Dataset, load_from_disk\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/jpic/geneformer_dev/scripts')\n",
    "import geneformer_utils as gtu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db0f940-3d8b-4699-aaba-bb188028e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from to_geneformer.py\n",
    "def check_counts_column(adata, counts_column):\n",
    "    \"\"\"Checks for and calculates a total counts column in AnnData.\n",
    "\n",
    "    This function examines the AnnData object's observation (`obs`) columns for the specified \n",
    "    `counts_column`. If it doesn't exist, the function calculates the sum of each row (cell) \n",
    "    across all features in the data matrix (`X`) and stores it as a new column in `obs`.\n",
    "\n",
    "    Args:\n",
    "        adata: An AnnData object containing the data to be analyzed.\n",
    "        counts_column: A string representing the desired name for the total counts column.\n",
    "\n",
    "    Returns:\n",
    "        adata: The modified AnnData object, now with the `counts_column` present (either \n",
    "               pre-existing or newly calculated).\n",
    "    \"\"\"\n",
    "    obs_columns = adata.obs.columns\n",
    "    \n",
    "    if counts_column in obs_columns:\n",
    "        return adata\n",
    "    else:\n",
    "        adata.obs[counts_column] = adata.X.sum(axis=1)\n",
    "        return adata\n",
    "    \n",
    "    \n",
    "def map_gene_names(adata, gene_id, gene_name_column, gene_names):\n",
    "    \"\"\"A function mapping gene names to gene ids \"\"\"\n",
    "    var_columns = adata.var.columns\n",
    "    \n",
    "    if gene_id in var_columns:\n",
    "        return adata\n",
    "    else:\n",
    "        adata.var[gene_id] = adata.var[gene_name_column].map(gene_names)\n",
    "        return adata\n",
    "    \n",
    "    \n",
    "def load_gene_names(gene_names_file):\n",
    "    \"\"\"\n",
    "    Loads a gene median dictionary from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        gene_names_file (str): Path to the pickle file containing the gene names dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping gene names to IDs\n",
    "    \"\"\"\n",
    "\n",
    "    with open(gene_names_file, \"rb\") as f:\n",
    "        gene_names_dict = pickle.load(f)\n",
    "\n",
    "    return gene_names_dict\n",
    "\n",
    "\n",
    "def load_gene_median_dict(gene_median_file):\n",
    "    \"\"\"\n",
    "    Loads a gene median dictionary from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        gene_median_file (str): Path to the pickle file containing the gene median dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping gene IDs to their median expression values.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(gene_median_file, \"rb\") as f:\n",
    "        gene_median_dict = pickle.load(f)\n",
    "\n",
    "    return gene_median_dict\n",
    "\n",
    "\n",
    "def load_gene_tokenization(token_dictionary_file):\n",
    "    \"\"\"\n",
    "    Loads gene tokenization data from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        token_dictionary_file (str): Path to the pickle file containing the gene-token dictionary.\n",
    "\n",
    "    Returns:\n",
    "        dict: Gene-token dictionary (Ensembl ID: token).\n",
    "        list: List of all gene keys (Ensembl IDs).\n",
    "        dict: Dictionary mapping gene keys to True (used for selecting genes later).\n",
    "    \"\"\"\n",
    "\n",
    "    with open(token_dictionary_file, \"rb\") as f:\n",
    "        gene_token_dict = pickle.load(f)\n",
    "\n",
    "    gene_keys = list(gene_token_dict.keys())\n",
    "\n",
    "    # Optimization: Pre-allocate the list for slight performance improvement\n",
    "    genelist_dict = dict.fromkeys(gene_keys, True)\n",
    "\n",
    "    return gene_token_dict, gene_keys, genelist_dict\n",
    "\n",
    "\n",
    "def rank_genes(gene_vector, gene_tokens):\n",
    "    \"\"\"Ranks genes based on expression values in descending order.\n",
    "\n",
    "    Args:\n",
    "        gene_vector (numpy.ndarray): Array of gene expression values.\n",
    "        gene_tokens (numpy.ndarray): Array of corresponding gene tokens.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of gene tokens sorted by descending expression value.\n",
    "    \"\"\"\n",
    "    return gene_tokens[np.argsort(-gene_vector)]\n",
    "\n",
    "\n",
    "def normalize_counts(adata_chunk,  counts_column='n_counts', target_sum=10000):\n",
    "    \"\"\"Normalizes gene expression counts within a chunk of AnnData.\n",
    "\n",
    "    Args:\n",
    "        adata_chunk (AnnData): A chunk of the AnnData object containing gene expression data.\n",
    "        counts_column (str): Name of the column in `adata_chunk.obs` containing the total counts per cell.\n",
    "        target_sum (float): The desired total count per cell after normalization.\n",
    "        norm_factor_vector (numpy.ndarray): An array of normalization factors for each gene.\n",
    "\n",
    "    Returns:\n",
    "        scipy.sparse.csr_matrix: A sparse matrix containing the normalized gene expression counts.\n",
    "\n",
    "    This function performs the following steps:\n",
    "        1. Extracts the total counts per cell from the specified column (`counts_column`).\n",
    "        2. Normalizes the gene expression matrix (`adata_chunk.X`) by dividing by the total counts \n",
    "           and multiplying by the `target_sum`.\n",
    "        3. Further adjusts the normalized values by dividing by the gene-specific normalization \n",
    "           factors (`norm_factor_vector`).\n",
    "        4. Returns the normalized expression matrix as a sparse CSR matrix for efficient storage \n",
    "           and computation.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_counts = adata_chunk.obs[counts_column].values[:, None]  # Cell counts as column vector\n",
    "    X_norm = adata_chunk.X / n_counts * target_sum / norm_factor_vector\n",
    "    return sp.csr_matrix(X_norm)  # Efficient sparse representation\n",
    "\n",
    "\n",
    "def tokenize_anndata(adata, genelist_dict, gene_median_dict, \n",
    "                     chunk_size=100000, target_sum=10000, \n",
    "                     counts_column='n_counts', gene_id=\"ensembl_id\", gene_token_dict=None):\n",
    "    \"\"\"\n",
    "    Tokenizes and ranks genes within an AnnData object, optimizing for memory efficiency.\n",
    "\n",
    "    This function processes gene expression data in chunks, applies normalization, and ranks genes\n",
    "    for each cell based on their expression levels. The resulting tokenized and ranked gene\n",
    "    representations, along with cell metadata, are returned.\n",
    "\n",
    "    Args:\n",
    "        adata (AnnData): The AnnData object containing gene expression data.\n",
    "        genelist_dict (dict): Dictionary mapping gene IDs to boolean values indicating relevance.\n",
    "        gene_median_dict (dict): Dictionary mapping gene IDs to their median expression values.\n",
    "        chunk_size (int, optional): Number of cells to process in each chunk (default: 1000).\n",
    "        target_sum (int, optional): Target sum for count normalization (default: 10000).\n",
    "        counts_column (str, optional): The column in `adata.obs` containing cell counts (default: 'n_counts').\n",
    "        gene_id (str, optional): The column in `adata.var` containing gene IDs (default: 'ensembl_id').\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - list: List of tokenized and ranked gene lists for each cell.\n",
    "            - dict: Dictionary containing cell metadata (keys are metadata column names).\n",
    "    \"\"\"\n",
    "    # Filter relevant miRNAs\n",
    "    coding_miRNA_mask = np.array([genelist_dict.get(i, False) for i in adata.var[gene_id]])\n",
    "    coding_miRNA_loc = np.where(coding_miRNA_mask)[0]\n",
    "\n",
    "    # Extract miRNA information\n",
    "    coding_miRNA_ids = adata.var[gene_id].iloc[coding_miRNA_loc]\n",
    "    norm_factor_vector = np.array([gene_median_dict[i] for i in coding_miRNA_ids])\n",
    "    coding_miRNA_tokens = np.array([gene_token_dict[i] for i in coding_miRNA_ids])\n",
    "\n",
    "    tokenized_cells = []\n",
    "    file_cell_metadata = {k: [] for k in adata.obs.columns}  # Initialize metadata dict\n",
    "\n",
    "    # Process in chunks for memory efficiency\n",
    "    for chunk_start in range(0, adata.shape[0], chunk_size):\n",
    "        chunk_end = chunk_start + chunk_size\n",
    "        adata_chunk = adata[chunk_start:chunk_end, coding_miRNA_loc]\n",
    "        \n",
    "        # Normalize counts (could be replaced with the untested function above)\n",
    "        n_counts = adata_chunk.obs[counts_column].values[:, None]\n",
    "        X_norm = adata_chunk.X / n_counts * target_sum / norm_factor_vector\n",
    "        X_norm = sp.csr_matrix(X_norm)  \n",
    "\n",
    "        # Tokenize and rank genes for each cell in chunk\n",
    "        for i in range(X_norm.shape[0]):\n",
    "            ranks = rank_genes(X_norm[i].data, coding_miRNA_tokens[X_norm[i].indices])\n",
    "            ranks = list(ranks[~np.isnan(ranks)].astype(int))\n",
    "\n",
    "            tokenized_cells.append(ranks)\n",
    "\n",
    "        # Update metadata\n",
    "        for k in adata.obs.columns:\n",
    "            file_cell_metadata[k].extend(adata_chunk.obs[k].astype(str).tolist())\n",
    "\n",
    "    return tokenized_cells, file_cell_metadata\n",
    "\n",
    "\n",
    "def format_cell_features(example):\n",
    "    \"\"\"\n",
    "    Truncates gene tokens (`input_ids`) to `model_size` and adds a `length` feature.\n",
    "\n",
    "    Args:\n",
    "        example (dict): Cell data with `input_ids` (list of gene tokens).\n",
    "\n",
    "    Returns:\n",
    "        dict: Modified cell data with truncated `input_ids` and added `length`.\n",
    "    \"\"\"\n",
    "    example[\"input_ids\"] = example[\"input_ids\"][0:model_size] \n",
    "    example[\"length\"] = len(example[\"input_ids\"]) \n",
    "    return example\n",
    "\n",
    "\n",
    "def save_hf_dataset(dataset: Dataset, output_path: str, overwrite=True):\n",
    "    \"\"\"\n",
    "    Saves a Hugging Face Dataset to disk at a specified file path.\n",
    "\n",
    "    This function serializes a Hugging Face `Dataset` object and saves it to disk in the Arrow format.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The Hugging Face `Dataset` object to be saved.\n",
    "        output_path (str): The full file path (including the filename) where the dataset will be saved. \n",
    "        overwrite (bool, optional): If `True`, an existing dataset at `output_path` will be overwritten. \n",
    "                                   If `False` and the file exists, a `FileExistsError` is raised (default: True).\n",
    "\n",
    "    Raises:\n",
    "        TypeError: If `dataset` is not a Hugging Face `Dataset` instance.\n",
    "        FileExistsError: If `output_path` points to an existing file and `overwrite` is False.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(dataset, Dataset):\n",
    "        raise TypeError(\"The provided dataset is not a Hugging Face Dataset.\")\n",
    "\n",
    "    if os.path.exists(output_path) and not overwrite:\n",
    "        raise FileExistsError(\n",
    "            f\"Dataset '{output_path}' already exists. Set `overwrite=True` to overwrite.\"\n",
    "        )\n",
    "    dataset.save_to_disk(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49242973-9c6a-4371-9188-21c0b2fe2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_file=None, output_directory=None, verbose=True):\n",
    "\n",
    "    input_path  = input_file\n",
    "    base_name   = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    output_path = os.path.join(output_directory, base_name + '.dataset')\n",
    "    outpath     = os.path.join(output_directory, base_name + '_GF_embedding.h5ad')\n",
    "    \n",
    "    # Default values\n",
    "    MODEL_PATH          = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer-12L-30M/\"\n",
    "    DEFAULT_NAME_PATH   = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer/gene_name_id_dict.pkl\"\n",
    "    DEFAULT_TOKEN_PATH  = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/token_dictionary.pkl\"\n",
    "    DEFAULT_MEDIAN_PATH = \"/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer/gene_median_dictionary.pkl\"\n",
    "    MODEL_INPUT_SIZE    = 2048\n",
    "    NUMBER_PROC         = 16\n",
    "    TARGET_SUM          = 10000\n",
    "    GENE_ID             = 'ensembl_id'\n",
    "    COUNTS_COLUMN       = 'n_counts'\n",
    "    LAYER               = 'X'\n",
    "    GENE_NAME_COLUMN    = 'gene_name'\n",
    "\n",
    "    # set values used for embedding\n",
    "    global model_size\n",
    "    token_path            = DEFAULT_TOKEN_PATH\n",
    "    median_path           = DEFAULT_MEDIAN_PATH\n",
    "    n_proc                = NUMBER_PROC\n",
    "    model_size            = MODEL_INPUT_SIZE\n",
    "    target_sum            = TARGET_SUM\n",
    "    gene_id               = GENE_ID\n",
    "    aggregate_transcripts = False\n",
    "    counts_column         = COUNTS_COLUMN\n",
    "    layer                 = LAYER\n",
    "    gene_names            = DEFAULT_NAME_PATH\n",
    "    gene_name_column      = GENE_NAME_COLUMN\n",
    "    map_names             = False\n",
    "    num_cells             = None # all cells, useful for testing \n",
    "\n",
    "\n",
    "    ###########################################\n",
    "    #\n",
    "    #   TOKENIZE COUNTS DATA FOR GENEFORMER\n",
    "    #\n",
    "    ###########################################\n",
    "    print(\"Loading gene tokenization data...\") if verbose else None\n",
    "    gene_token_dict, gene_keys, genelist_dict = load_gene_tokenization(token_path)\n",
    "    print(f\"Loaded {len(gene_token_dict)} gene tokens\") if verbose else None\n",
    "    \n",
    "    print(\"Loading gene median expression data...\") if verbose else None\n",
    "    gene_median_dict = load_gene_median_dict(median_path)\n",
    "    print(f\"Loaded {len(gene_median_dict)} gene median expression values\") if verbose else None\n",
    "    \n",
    "    if map_names:\n",
    "        print(\"Loading gene name mapping data...\") if verbose else None\n",
    "        gene_names = load_gene_names(gene_names)\n",
    "        print(f\"Loaded {len(gene_names)} gene name mappings\") if verbose else None\n",
    "    \n",
    "    # Load and pre-process data\n",
    "    print(f\"Loading AnnData from {input_path}...\") if verbose else None\n",
    "    adata = sc.read_h5ad(input_path)\n",
    "    print(f\"Loaded AnnData with shape {adata.shape}\") if verbose else None\n",
    "    \n",
    "    if map_names:\n",
    "        print(\"Mapping gene names to Ensembl IDs...\") if verbose else None\n",
    "        adata = map_gene_names(adata, gene_id, gene_name_column, gene_names)\n",
    "    \n",
    "    if not layer == 'X':\n",
    "        print(f\"Using layer '{layer}' for expression data...\") if verbose else None\n",
    "        adata.X = adata.layers[layer]\n",
    "        \n",
    "    print(\"Checking for and/or calculating total counts per cell...\") if verbose else None\n",
    "    adata = check_counts_column(adata, counts_column)\n",
    "    \n",
    "    # Tokenize and rank genes\n",
    "    print(\"Tokenizing and ranking genes...\") if verbose else None\n",
    "    tokenized_cells, cell_metadata = tokenize_anndata(\n",
    "        adata, genelist_dict, gene_median_dict,\n",
    "        target_sum=target_sum, gene_id=gene_id, counts_column=counts_column,\n",
    "        gene_token_dict=gene_token_dict\n",
    "    )\n",
    "    print(f\"Processed {len(tokenized_cells)} cells\") if verbose else None\n",
    "    \n",
    "    # Create Hugging Face dataset\n",
    "    print(\"Creating Hugging Face dataset...\") if verbose else None\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": tokenized_cells,\n",
    "        **cell_metadata\n",
    "    }\n",
    "    output_dataset = Dataset.from_dict(dataset_dict)\n",
    "    print(f\"Dataset has {len(output_dataset)} examples\") if verbose else None\n",
    "    \n",
    "    # Format cell features\n",
    "    print(\"Formatting cell features...\") if verbose else None\n",
    "    dataset = output_dataset.map(format_cell_features, num_proc=n_proc)\n",
    "    \n",
    "    # Save dataset\n",
    "    print(f\"Saving processed dataset to {output_path}...\") if verbose else None\n",
    "    \n",
    "    save_hf_dataset(dataset, output_path, overwrite=True)\n",
    "    print(\"Processing completed successfully!\") if verbose else None\n",
    "\n",
    "    ###########################################\n",
    "    #\n",
    "    #   EMBED TOKENS WITH GENEFORMER TO ANNDATA\n",
    "    #\n",
    "    ###########################################\n",
    "    dataset_path = output_path\n",
    "    \n",
    "    print(MODEL_PATH)\n",
    "    \n",
    "    print(f\"Loading model from '{MODEL_PATH}'...\") if verbose else None\n",
    "    model = gtu.load_model(MODEL_PATH)\n",
    "    print(\"Model loaded successfully!\") if verbose else None\n",
    "    \n",
    "    print(f\"Loading dataset from '{dataset_path}' (up to {num_cells} cells)...\") if verbose else None\n",
    "    try:\n",
    "        df = gtu.load_data_as_dataframe(dataset_path, num_cells=num_cells)\n",
    "        data = Dataset.from_pandas(df)\n",
    "        df = df.drop(columns='input_ids')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file not found at '{dataset_path}'\") if verbose else None\n",
    "        sys.exit(1)\n",
    "    except Exception as e:  # Catching other potential errors\n",
    "        print(f\"Error loading dataset: {e}\") if verbose else None\n",
    "        sys.exit(1)\n",
    "    print(\"Dataset loaded successfully!\") if verbose else None\n",
    "    \n",
    "    print(\"Extracting embeddings...\") if verbose else None\n",
    "    embs = gtu.extract_embedding_in_mem(model, data)\n",
    "    adata = gtu.embedding_to_adata(embs)\n",
    "    adata.obs = df.astype(str).reset_index().copy()\n",
    "    print(\"Embeddings extracted successfully!\") if verbose else None\n",
    "    \n",
    "    print(f\"Writing results to '{outpath}'...\") if verbose else None\n",
    "    try:\n",
    "        adata.write(outpath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing output file: {e}\") if verbose else None\n",
    "        sys.exit(1)\n",
    "    print(\"Output file written successfully!\") if verbose else None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "419d94c3-19ef-40af-88e2-e87629e37620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gene tokenization data...\n",
      "Loaded 25426 gene tokens\n",
      "Loading gene median expression data...\n",
      "Loaded 25424 gene median expression values\n",
      "Loading AnnData from /nfs/turbo/umms-indikar/shared/projects/geneformer/data/rajapakse_lab_data_jpic.h5ad...\n",
      "Loaded AnnData with shape (66, 19393)\n",
      "Checking for and/or calculating total counts per cell...\n",
      "Tokenizing and ranking genes...\n",
      "Processed 66 cells\n",
      "Creating Hugging Face dataset...\n",
      "Dataset has 66 examples\n",
      "Formatting cell features...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f8db45e13d34e44aa15e8244adbe050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed dataset to /nfs/turbo/umms-indikar/shared/projects/geneformer/data/test/rajapakse_lab_data_jpic.dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63dd0ea3bc4d44c69d5f984a5b5463d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/66 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed successfully!\n",
      "/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer-12L-30M/\n",
      "Loading model from '/nfs/turbo/umms-indikar/shared/projects/geneformer/geneformer-12L-30M/'...\n",
      "Model loaded successfully!\n",
      "Loading dataset from '/nfs/turbo/umms-indikar/shared/projects/geneformer/data/test/rajapakse_lab_data_jpic.dataset' (up to None cells)...\n",
      "Dataset loaded successfully!\n",
      "Extracting embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44f1b83ad8a44789f8c164b51e5066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings extracted successfully!\n",
      "Writing results to '/nfs/turbo/umms-indikar/shared/projects/geneformer/data/test/rajapakse_lab_data_jpic_GF_embedding.h5ad'...\n",
      "Output file written successfully!\n"
     ]
    }
   ],
   "source": [
    "arg_in  = '/nfs/turbo/umms-indikar/shared/projects/geneformer/data/rajapakse_lab_data_jpic.h5ad'\n",
    "arg_out = '/nfs/turbo/umms-indikar/shared/projects/geneformer/data/test'\n",
    "main(input_file=arg_in, output_directory=arg_out, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af208707-42ae-4e4c-8d0f-8244fd0a9028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpic/.local/lib/python3.11/site-packages/anndata/__init__.py:55: FutureWarning: `anndata.read` is deprecated, use `anndata.read_h5ad` instead. `ad.read` will be removed in mid 2024.\n",
      "  warnings.warn(\n",
      "/home/jpic/.local/lib/python3.11/site-packages/anndata/_core/aligned_df.py:67: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "ad = an.read('/nfs/turbo/umms-indikar/shared/projects/geneformer/data/test/rajapakse_lab_data_jpic.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba03ee14-52cd-4e84-a863-5c4886e32460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 66 × 512\n",
       "    obs: 'index', 'dataset', 'sample_id', 'timepoint', 'hour', 'n_counts', 'control', 'order', 'replicate', 'batch', 'length'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a093f5d-0073-42c7-902b-b208a31140ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dataset</th>\n",
       "      <th>sample_id</th>\n",
       "      <th>timepoint</th>\n",
       "      <th>hour</th>\n",
       "      <th>n_counts</th>\n",
       "      <th>control</th>\n",
       "      <th>order</th>\n",
       "      <th>replicate</th>\n",
       "      <th>batch</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chen_2015</td>\n",
       "      <td>S1a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7901832</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chen_2015</td>\n",
       "      <td>S1b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8113329</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chen_2015</td>\n",
       "      <td>S2a</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9831046</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chen_2015</td>\n",
       "      <td>S2b</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10123271</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chen_2015</td>\n",
       "      <td>S3a</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10490839</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>liu_2018</td>\n",
       "      <td>63275</td>\n",
       "      <td>3.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>13515971</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>liu_2018</td>\n",
       "      <td>63290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>9522866</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>63</td>\n",
       "      <td>liu_2018</td>\n",
       "      <td>63287</td>\n",
       "      <td>2.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>12370157</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>liu_2018</td>\n",
       "      <td>63284</td>\n",
       "      <td>3.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>10970735</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>liu_2018</td>\n",
       "      <td>63293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>13244720</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index    dataset sample_id timepoint   hour  n_counts control order  \\\n",
       "0       0  chen_2015       S1a       0.0    0.0   7901832    True     1   \n",
       "1       1  chen_2015       S1b       0.0    0.0   8113329    True     1   \n",
       "2       2  chen_2015       S2a       0.0    0.0   9831046   False     2   \n",
       "3       3  chen_2015       S2b       0.0    0.0  10123271   False     2   \n",
       "4       4  chen_2015       S3a       1.0    8.0  10490839   False     3   \n",
       "..    ...        ...       ...       ...    ...       ...     ...   ...   \n",
       "61     61   liu_2018     63275       3.0   80.0  13515971   False    11   \n",
       "62     62   liu_2018     63290       1.0   88.0   9522866   False    12   \n",
       "63     63   liu_2018     63287       2.0   96.0  12370157   False    13   \n",
       "64     64   liu_2018     63284       3.0  104.0  10970735   False    14   \n",
       "65     65   liu_2018     63293       1.0  112.0  13244720   False    15   \n",
       "\n",
       "   replicate batch length  \n",
       "0          1     0   2048  \n",
       "1          2     0   2048  \n",
       "2          1     0   2048  \n",
       "3          2     0   2048  \n",
       "4          1     0   2048  \n",
       "..       ...   ...    ...  \n",
       "61         3     1   2048  \n",
       "62         3     1   2048  \n",
       "63         3     1   2048  \n",
       "64         3     1   2048  \n",
       "65         3     1   2048  \n",
       "\n",
       "[66 rows x 11 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.obs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
